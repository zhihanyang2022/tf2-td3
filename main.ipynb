{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:13:29.669298Z",
     "start_time": "2021-09-23T02:13:29.664342Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "import gym\n",
    "from gym.wrappers import RescaleAction\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor and critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:12:17.249030Z",
     "start_time": "2021-09-23T02:12:17.228612Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_MLP(num_in, num_out, final_activation, hidden_dimensions=(256, 256)):\n",
    "\n",
    "    tensor_dimensions = [num_in]\n",
    "    if hidden_dimensions is not None:\n",
    "        tensor_dimensions.extend(hidden_dimensions)\n",
    "    if num_out is not None:\n",
    "        tensor_dimensions.append(num_out)\n",
    "\n",
    "    num_layers = len(tensor_dimensions)  # now including the input layer\n",
    "    list_of_layers = []\n",
    "\n",
    "    # tf uses lazy instantiation, so input dimension is inferred during forward pass\n",
    "\n",
    "    for i, output_dimension in enumerate(tensor_dimensions):\n",
    "        if i == 0:\n",
    "            list_of_layers.append(tf.keras.Input(output_dimension))\n",
    "        elif i == num_layers - 1:\n",
    "            if final_activation is None:\n",
    "                list_of_layers.append(tf.keras.layers.Dense(output_dimension))\n",
    "            else:\n",
    "                list_of_layers.append(tf.keras.layers.Dense(output_dimension, activation=final_activation))\n",
    "        else:\n",
    "            list_of_layers.append(tf.keras.layers.Dense(output_dimension, activation='relu'))\n",
    "    net = keras.Sequential(list_of_layers)\n",
    "\n",
    "    return net  # actual_num_out is not required\n",
    "\n",
    "\n",
    "class MLPTanhActor(keras.Model):\n",
    "    \"\"\"Output actions from [-1, 1].\"\"\"\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = make_MLP(num_in=input_dim, num_out=action_dim, final_activation='tanh')\n",
    "        self.build(input_shape=(None, input_dim))  # create the parameters within init based on call; crucial\n",
    "\n",
    "    def call(self, states: tf.Tensor):\n",
    "        return self.net(states)\n",
    "\n",
    "\n",
    "class MLPCritic(keras.Model):\n",
    "\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = make_MLP(num_in=input_dim + action_dim, num_out=1, final_activation=None)\n",
    "        self.build(input_shape=(None, input_dim + action_dim))\n",
    "\n",
    "    def call(self, states_and_actions: tuple):\n",
    "        return self.net(tf.concat(states_and_actions, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:12:17.269011Z",
     "start_time": "2021-09-23T02:12:17.253359Z"
    }
   },
   "outputs": [],
   "source": [
    "def polyak_update(targ_net: keras.Model, pred_net: keras.Model, polyak: float) -> None:\n",
    "    for i in range(len(pred_net.weights)):\n",
    "        targ_net.weights[i].assign(tf.scalar_mul(1 - polyak, pred_net.weights[i]) + tf.scalar_mul(polyak, targ_net.weights[i]))\n",
    "\n",
    "def save_net(net: keras.Model, save_dir: str, save_name: str) -> None:\n",
    "    net.save_weights(os.path.join(save_dir, save_name))\n",
    "\n",
    "def load_net(net: keras.Model, save_dir: str, save_name: str) -> None:\n",
    "    net.load_weights(os.path.join(save_dir, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:14:44.227385Z",
     "start_time": "2021-09-23T02:14:44.190460Z"
    }
   },
   "outputs": [],
   "source": [
    "ERROR_DATA = [\"shape mismatch\"]\n",
    "\n",
    "class TD3():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        action_dim,\n",
    "        gamma=0.99,\n",
    "        lr=3e-4,\n",
    "        lr_schedule=None,\n",
    "        polyak=0.995,\n",
    "        action_noise=0.1,  # standard deviation of action noise\n",
    "        target_noise=0.2,  # standard deviation of target smoothing noise\n",
    "        noise_clip=0.5,  # max abs value of target smoothing noise\n",
    "        policy_delay=2\n",
    "    ):\n",
    "\n",
    "        # hyper-parameters\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.polyak = polyak\n",
    "\n",
    "        self.action_noise = action_noise\n",
    "        self.target_noise = target_noise\n",
    "        self.noise_clip = noise_clip\n",
    "\n",
    "        self.policy_delay = policy_delay\n",
    "\n",
    "        # trackers\n",
    "\n",
    "        self.num_Q_updates = tf.Variable(0)  # for delaying updates\n",
    "        self.mean_Q1_value = tf.Variable(0, dtype=tf.float32)  # for logging; the actor does not get updated every iteration,\n",
    "        # so this statistic is not available every iteration\n",
    "\n",
    "        # networks\n",
    "        # (keras.models.clone_model cannot be used for subclassed models)\n",
    "        # (weirdly, the weights must be converted to numpy for set_weights to work)\n",
    "\n",
    "        self.actor = MLPTanhActor(input_dim, action_dim)\n",
    "        self.actor_targ = MLPTanhActor(input_dim, action_dim)\n",
    "        self.actor_targ.set_weights([w.numpy() for w in self.actor.weights])\n",
    "\n",
    "        self.Q1 = MLPCritic(input_dim, action_dim)\n",
    "        self.Q1_targ = MLPCritic(input_dim, action_dim)\n",
    "        self.Q1_targ.set_weights([w.numpy() for w in self.Q1.weights])\n",
    "\n",
    "        self.Q2 = MLPCritic(input_dim, action_dim)\n",
    "        self.Q2_targ = MLPCritic(input_dim, action_dim)\n",
    "        self.Q2_targ.set_weights([w.numpy() for w in self.Q2.weights])\n",
    "\n",
    "        # optimizers\n",
    "\n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.Q1_optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.Q2_optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    @tf.function\n",
    "    def act(self, state: np.array, deterministic: bool) -> np.array:\n",
    "        state_with_batch_dim = tf.reshape(state, (1, -1))\n",
    "        greedy_action = tf.reshape(self.actor(state_with_batch_dim), (-1, ))\n",
    "        if deterministic:\n",
    "            return greedy_action\n",
    "        else:\n",
    "            return tf.clip_by_value(greedy_action + self.action_noise * np.random.randn(self.action_dim), -1.0, 1.0)\n",
    "\n",
    "    @tf.function\n",
    "    def update_networks(self, b, debug=False):\n",
    "\n",
    "        if debug:\n",
    "            bs = b.ns.shape[0]  # for shape checking\n",
    "\n",
    "        # compute targets\n",
    "\n",
    "        na = self.actor_targ(b.ns)\n",
    "        noise = tf.clip_by_value(\n",
    "            tf.random.normal(na.shape) * self.target_noise, -self.noise_clip, self.noise_clip\n",
    "        )\n",
    "        smoothed_na = tf.clip_by_value(na + noise, -1, 1)\n",
    "\n",
    "        n_min_Q_targ = tf.math.minimum(self.Q1_targ((b.ns, smoothed_na)), self.Q2_targ((b.ns, smoothed_na)))\n",
    "\n",
    "        targets = b.r + self.gamma * (1 - b.d) * n_min_Q_targ\n",
    "\n",
    "        if debug:\n",
    "            tf.Assert(na.shape == (bs, self.action_dim), ERROR_DATA)\n",
    "            tf.Assert(n_min_Q_targ.shape == (bs, 1), ERROR_DATA)\n",
    "            tf.Assert(targets.shape == (bs, 1), ERROR_DATA)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # compute predictions\n",
    "\n",
    "            Q1_predictions = self.Q1((b.s, b.a))\n",
    "            Q2_predictions = self.Q2((b.s, b.a))\n",
    "\n",
    "            # compute td error\n",
    "\n",
    "            Q1_loss = tf.reduce_mean((Q1_predictions - targets) ** 2)\n",
    "            Q2_loss = tf.reduce_mean((Q2_predictions - targets) ** 2)\n",
    "\n",
    "        if debug:\n",
    "            tf.Assert(Q1_loss.shape == (), ERROR_DATA)\n",
    "            tf.Assert(Q2_loss.shape == (), ERROR_DATA)\n",
    "\n",
    "        # reduce td error\n",
    "\n",
    "        Q1_gradients = tape.gradient(Q1_loss, self.Q1.trainable_weights)\n",
    "        self.Q1_optimizer.apply_gradients(zip(Q1_gradients, self.Q1.trainable_weights))\n",
    "\n",
    "        Q2_gradients = tape.gradient(Q2_loss, self.Q2.trainable_weights)\n",
    "        self.Q2_optimizer.apply_gradients(zip(Q2_gradients, self.Q2.trainable_weights))\n",
    "\n",
    "        self.num_Q_updates.assign_add(1)\n",
    "\n",
    "        if self.num_Q_updates % self.policy_delay == 0:  # delayed policy update\n",
    "\n",
    "            # compute policy loss\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                a = self.actor(b.s)\n",
    "                Q1_values = self.Q1((b.s, a))\n",
    "                policy_loss = - tf.reduce_mean(Q1_values)\n",
    "\n",
    "            self.mean_Q1_value.assign(-policy_loss)  # logging purpose only\n",
    "            if debug:\n",
    "                tf.Assert(a.shape == (bs, self.action_dim), ERROR_DATA)\n",
    "                tf.Assert(Q1_values.shape == (bs, 1), ERROR_DATA)\n",
    "                tf.Assert(policy_loss.shape == (), ERROR_DATA)\n",
    "\n",
    "            # reduce policy loss\n",
    "\n",
    "            policy_gradients = tape.gradient(policy_loss, self.actor.trainable_weights)\n",
    "            self.actor_optimizer.apply_gradients(zip(policy_gradients, self.actor.trainable_weights))\n",
    "\n",
    "            # update target networks\n",
    "\n",
    "            polyak_update(targ_net=self.actor_targ, pred_net=self.actor, polyak=self.polyak)\n",
    "            polyak_update(targ_net=self.Q1_targ, pred_net=self.Q1, polyak=self.polyak)\n",
    "            polyak_update(targ_net=self.Q2_targ, pred_net=self.Q2, polyak=self.polyak)\n",
    "\n",
    "        return {\n",
    "            # for learning the q functions\n",
    "            '(qfunc) Q1 pred': tf.reduce_mean(Q1_predictions),\n",
    "            '(qfunc) Q2 pred': tf.reduce_mean(Q2_predictions),\n",
    "            '(qfunc) Q1 loss': Q1_loss,\n",
    "            '(qfunc) Q2 loss': Q2_loss,\n",
    "            # for learning the actor\n",
    "            '(actor) Q1 value': self.mean_Q1_value\n",
    "        }\n",
    "\n",
    "    def save_actor(self, save_dir: str) -> None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_net(net=self.actor, save_dir=save_dir, save_name=\"actor.h5\")\n",
    "\n",
    "    def load_actor(self, save_dir: str) -> None:\n",
    "        load_net(net=self.actor, save_dir=save_dir, save_name=\"actor.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:14:45.392831Z",
     "start_time": "2021-09-23T02:14:45.387530Z"
    }
   },
   "outputs": [],
   "source": [
    "Batch = namedtuple('Batch', 's a r ns d')\n",
    "Transition = namedtuple('Transition', 's a r ns d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:14:45.612448Z",
     "start_time": "2021-09-23T02:14:45.599866Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Just a standard FIFO replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity=int(1e6), batch_size=100):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, s, a, r, ns, d) -> None:\n",
    "        self.memory.appendleft(Transition(s, a, r, ns, d))\n",
    "\n",
    "    def is_ready(self):\n",
    "        return len(self.memory) >= self.batch_size\n",
    "\n",
    "    def sample(self) -> Batch:\n",
    "        transitions = random.choices(self.memory, k=self.batch_size)  # sampling WITH replacement\n",
    "        batch_raw = Batch(*zip(*transitions))\n",
    "        # actually, converting to tf tensor is not necessary here; could have just used numpy reshape and astype\n",
    "        s = tf.reshape(tf.convert_to_tensor(batch_raw.s, dtype=tf.float32), (self.batch_size, -1))\n",
    "        a = tf.reshape(tf.convert_to_tensor(batch_raw.a, dtype=tf.float32), (self.batch_size, -1))\n",
    "        r = tf.reshape(tf.convert_to_tensor(batch_raw.r, dtype=tf.float32), (self.batch_size, 1))\n",
    "        ns = tf.reshape(tf.convert_to_tensor(batch_raw.ns, dtype=tf.float32), (self.batch_size, -1))\n",
    "        d = tf.reshape(tf.convert_to_tensor(batch_raw.d, dtype=tf.float32), (self.batch_size, 1))\n",
    "        return Batch(s, a, r, ns, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test\n",
    "\n",
    "Learning from a random batch for 1000 iterations; shouldn't take more than 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:14:52.116381Z",
     "start_time": "2021-09-23T02:14:47.174628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.694819589999952"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm = TD3(input_dim=2, action_dim=1)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "batch = Batch(\n",
    "    s=np.random.randn(batch_size, 2).astype('float32'), \n",
    "    a=np.random.randn(batch_size, 1).astype('float32'),\n",
    "    r=np.random.randn(batch_size, 1).astype('float32'),\n",
    "    ns=np.random.randn(batch_size, 2).astype('float32'),\n",
    "    d=np.zeros((batch_size, 1)).astype('float32')\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(1000):\n",
    "    stats = algorithm.update_networks(batch)\n",
    "time.perf_counter() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Pendulum-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:16:19.355620Z",
     "start_time": "2021-09-23T02:15:17.713038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -1377.7201519550424 1.794488460000025\n",
      "2 -1529.3996065565661 1.0556090200000199\n",
      "3 -1456.6756359985886 1.0967808179999565\n",
      "4 -1344.584958734682 1.1548143249999612\n",
      "5 -1539.4519860086668 1.1703114599999935\n",
      "6 -1208.306171330558 1.0985516450000432\n",
      "7 -1222.7494143507333 1.1925283159999935\n",
      "8 -1258.2740468115094 1.2040690029999723\n",
      "9 -1298.5261874564592 1.2209370260000014\n",
      "10 -1289.4815854038097 1.2726620660000663\n",
      "11 -1602.7285927115747 1.1414879430000155\n",
      "12 -1329.5986483250476 1.0790147920000663\n",
      "13 -1267.603610474868 1.0285639100000026\n",
      "14 -1557.8745009175545 1.1278508709999642\n",
      "15 -1060.1218081699435 1.0668025140000736\n",
      "16 -1068.9396667373805 1.0879064770000468\n",
      "17 -1302.2124557656587 1.1822623059999842\n",
      "18 -1098.2811893056407 1.2260191759999088\n",
      "19 -1082.3421652002253 1.2925586159999511\n",
      "20 -934.9824548898551 1.0865474239999457\n",
      "21 -895.2772368792777 1.255021227000043\n",
      "22 -747.08303465816 1.1789143299999978\n",
      "23 -780.4148414479563 1.173073058\n",
      "24 -912.0859684696242 1.0808101179999312\n",
      "25 -656.3590619618615 1.154936895999981\n",
      "26 -736.4524110604989 1.3765052979999837\n",
      "27 -803.8880437999167 1.2133213770000566\n",
      "28 -641.1473722636548 1.2173453709999649\n",
      "29 -770.2997168586413 1.270043681000061\n",
      "30 -513.922093773493 1.250654735000012\n",
      "31 -640.355363450754 1.3213615690000324\n",
      "32 -607.1948815042323 1.3364085829999794\n",
      "33 -715.9016004999048 1.317160436999984\n",
      "34 -694.8427022111665 1.3963522669999975\n",
      "35 -591.6965252446117 1.3272847010000532\n",
      "36 -707.3258486293189 1.229184465000003\n",
      "37 -516.9379703197122 1.0716187419999414\n",
      "38 -573.7008712297023 1.2549125020000247\n",
      "39 -518.838801868584 1.3055741379999972\n",
      "40 -265.2060127943861 1.3300287929999968\n",
      "41 -260.26340426039445 1.2828277929999103\n",
      "42 -144.77548399766107 1.285715988999982\n",
      "43 -257.486359186374 1.3435676120000153\n",
      "44 -371.3039098106827 1.1754683359999945\n",
      "45 -452.93250847378056 1.2901681589999043\n",
      "46 -143.88697400475473 1.2402550360000077\n",
      "47 -142.76949601178862 1.3105458899999576\n",
      "48 -2.5176193093732078 1.2420901159999858\n",
      "49 -355.4518672024659 1.2698376069999995\n",
      "50 -118.4220136710416 1.25453509700003\n"
     ]
    }
   ],
   "source": [
    "env = RescaleAction(gym.make(\"Pendulum-v0\"), -1, 1)\n",
    "algorithm = TD3(input_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "buffer = ReplayBuffer()\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.perf_counter()\n",
    "    obs = env.reset()\n",
    "    ret = 0\n",
    "    while True:\n",
    "        action = algorithm.act(obs, deterministic=False).numpy()\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        ret += reward\n",
    "        buffer.push(obs, action, reward, next_obs, False)\n",
    "        if buffer.is_ready():\n",
    "            algorithm.update_networks(buffer.sample())\n",
    "        if done:\n",
    "            break\n",
    "        obs = next_obs\n",
    "    print(epoch + 1, ret, time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:16:31.669326Z",
     "start_time": "2021-09-23T02:16:31.653756Z"
    }
   },
   "outputs": [],
   "source": [
    "algorithm.save_actor(save_dir=\"./saved_models/pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:16:31.867515Z",
     "start_time": "2021-09-23T02:16:31.854847Z"
    }
   },
   "outputs": [],
   "source": [
    "algorithm.load_actor(save_dir=\"./saved_models/pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:17:01.966158Z",
     "start_time": "2021-09-23T02:16:58.864734Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action = algorithm.act(obs, deterministic=False).numpy()\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "    obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2_td3)",
   "language": "python",
   "name": "tf2_td3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
